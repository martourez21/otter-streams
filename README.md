#  Otter-Streams

> Production-grade machine learning inference for Apache Flink

[![Java CI](https://github.com/martourez21/otter-streams/actions/workflows/ci.yml/badge.svg)](https://github.com/martourez21/otter-streams/actions/workflows/ci.yml)
[![Maven Central](https://img.shields.io/maven-central/v/com.codedstreams/otter-streams)](https://search.maven.org/artifact/com.codedstreams/otter-streams)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![GitHub stars](https://img.shields.io/github/stars/martourez21/otter-streams?style=social)](https://github.com/martourez21/otter-streams/stargazers)
[![Discussions](https://img.shields.io/badge/Discussions-Enabled-blue)](https://github.com/martourez21/otter-streams/discussions)

<div align="center">
  <img src="docs/assets/otterstream-sdk-icon.ico" alt="Otter Streams Logo" width="120"/>
</div>

---

## âœ¨ What is Otter Streams?

Otter Streams is an open-source library that brings production-grade machine learning inference to Apache Flink streaming applications. Deploy your ML models externally or directly into Flink pipelines with enterprise-grade performance, reliability, and monitoring.

### Why choose Otter Streams?

- **ğŸš€ Real-time ML at scale** - Perform inference on streaming data with millisecond latency
- **ğŸ”Œ Framework agnostic** - Support for ONNX, TensorFlow, PyTorch, XGBoost, and PMML
- **ğŸ¢ Production ready** - Built-in monitoring, caching, and error handling
- **â˜ï¸ Deployment flexibility** - Local execution, cloud services, or hybrid deployments
- **ğŸ“Š Full observability** - Comprehensive metrics and monitoring out of the box

## ğŸ¯ Quick Start

### Add to Your Project

```xml
<dependency>
    <groupId>com.codedstreams</groupId>
    <artifactId>ml-inference-core</artifactId>
    <version>1.0.0</version>
</dependency>
```

### Basic Example

```java
// Add ML inference to your Flink stream in minutes
DataStream<FraudScore> predictions = transactionStream
    .map(new AsyncModelInferenceFunction<>(config));
```

**ğŸ“– [See the Getting Started Guide](GETTING_STARTED.md)** for detailed instructions.

## ğŸ“š Documentation

- **[ğŸ“– Getting Started](GETTING_STARTED.md)** - Your first inference pipeline
- **[ğŸ—ï¸ Architecture Overview](ARCHITECTURE.md)** - System design and components
- **[ğŸ¯ Examples & Use Cases](EXAMPLES.md)** - Real-world implementation patterns
- **[ğŸ”§ API Reference](https://martourez21.github.io/otter-streams/javadoc/)** - Complete API documentation
- **[ğŸ“Š Performance Guide](PERFORMANCE.md)** - Optimization and tuning

## ğŸŒŸ Key Features

| Feature | Description |
|---------|-------------|
| **Multi-Framework Support** | Run models from ONNX, TensorFlow, PyTorch, XGBoost, and PMML |
| **Async & High Performance** | Non-blocking execution with intelligent batching |
| **Enterprise Monitoring** | Built-in metrics, logging, and health checks |
| **Flexible Deployment** | Local, cloud, or hybrid inference strategies |
| **Production Resilience** | Retry logic, caching, and error handling |

## ğŸ¢ Use Cases

### Real-time Fraud Detection
```java
// Detect fraudulent transactions as they occur
DataStream<FraudScore> scores = transactionStream
    .process(new FraudDetectionModel());
```

### Personalized Recommendations
```java
// Generate personalized content in real-time
DataStream<Recommendation> recs = userBehaviorStream
    .process(new RecommendationModel());
```

### Anomaly Detection
```java
// Monitor systems and detect anomalies immediately
DataStream<AnomalyScore> anomalies = sensorStream
    .process(new AnomalyDetectionModel());
```

**ğŸ” [Explore more use cases](EXAMPLES.md)**

## ğŸ›  Supported Frameworks

<table>
<tr>
<td align="center" width="20%">
<img src="https://raw.githubusercontent.com/martourez21/otter-streams/main/docs/assets/onnx-logo.png" alt="ONNX" width="40"/>
<br/>
<strong>ONNX Runtime</strong>
</td>
<td align="center" width="20%">
<img src="https://raw.githubusercontent.com/martourez21/otter-streams/main/docs/assets/tensorflow-logo.png" alt="TensorFlow" width="40"/>
<br/>
<strong>TensorFlow</strong>
</td>
<td align="center" width="20%">
<img src="https://raw.githubusercontent.com/martourez21/otter-streams/main/docs/assets/pytorch-logo.png" alt="PyTorch" width="40"/>
<br/>
<strong>PyTorch</strong>
</td>
<td align="center" width="20%">
<img src="https://raw.githubusercontent.com/martourez21/otter-streams/main/docs/assets/xgboost-logo.png" alt="XGBoost" width="40"/>
<br/>
<strong>XGBoost</strong>
</td>
<td align="center" width="20%">
<img src="https://raw.githubusercontent.com/martourez21/otter-streams/main/docs/assets/pmml-logo.png" alt="PMML" width="40"/>
<br/>
<strong>PMML</strong>
</td>
</tr>
</table>

**ğŸ”— [See Framework Integration Details](ARCHITECTURE.md)**

## ğŸ“¦ Project Structure

```
otter-streams/
â”œâ”€â”€ ml-inference-core/          # Core inference engine
â”œâ”€â”€ otter-stream-onnx/         # ONNX Runtime integration
â”œâ”€â”€ otter-stream-tensorflow/   # TensorFlow SavedModel support
â”œâ”€â”€ otter-stream-pytorch/      # PyTorch model inference
â”œâ”€â”€ otter-stream-xgboost/      # XGBoost integration
â”œâ”€â”€ otter-stream-pmml/         # PMML model support
â”œâ”€â”€ otter-stream-remote/       # Remote inference service
â”œâ”€â”€ otter-stream-examples/     # Usage examples
â””â”€â”€ docs/                      # Documentation
```

**ğŸ—ï¸ [Learn about the architecture](ARCHITECTURE.md)**

## ğŸ¤ Community & Support

### Get Help
- **ğŸ“– [Documentation](https://martourez21.github.io/otter-streams/)** - Complete user guide
- **ğŸ’¬ [GitHub Discussions](https://github.com/martourez21/otter-streams/discussions)** - Questions and ideas
- **ğŸ› [Issue Tracker](https://github.com/martourez21/otter-streams/issues)** - Bug reports and feature requests
- **ğŸ“§ [Email Support](mailto:nestorabiawuh@gmail.com)** - Direct contact

### Stay Updated
- â­ **Star the repository** to show your support
- ğŸ‘€ **Watch releases** to get notifications
- ğŸ”„ **Follow updates** on GitHub

## ğŸ‘¥ Contributing

We love our contributors! Whether you're fixing bugs, improving documentation, or adding new features, all contributions are welcome.

**ğŸ“‹ [Read our Contributing Guide](CONTRIBUTING.md)**

### Quick Start for Contributors
```bash
# 1. Fork and clone
git clone https://github.com/your-username/otter-streams.git

# 2. Build the project
mvn clean install

# 3. Run tests
mvn test
```

### Ways to Contribute
- ğŸ› **Report bugs** and issues
- ğŸ’¡ **Suggest features** and improvements
- ğŸ“š **Improve documentation**
- ğŸ”§ **Fix issues** labeled "good first issue"
- ğŸ§ª **Add tests** and examples
- ğŸŒ **Help others** in discussions

## ğŸ“„ License

Otter Streams is licensed under the **Apache License 2.0**. See the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgements

Built with Passion by [Nestor Martourez](https://github.com/martourez21) and the open-source community. Special thanks to:

- The Apache Flink community
- All our contributors and users
- Open-source ML framework maintainers

---

<div align="center">

**Ready to add ML to your streaming pipelines?**

[ğŸ“– Get Started](GETTING_STARTED.md) Â· [ğŸ’¬ Join Discussions](https://github.com/martourez21/otter-streams/discussions) Â· [â­ Star the Project](https://github.com/martourez21/otter-streams/stargazers)

</div>